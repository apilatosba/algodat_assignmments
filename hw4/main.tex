\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{array}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\begin{document}
\section*{\huge Homework Sheet 4}
\begin{flushright}
  \begin{tabular}{@{} l l l @{}}
    \textbf{Author} & \textbf{Matriculation Number} & \textbf{Tutor} \\
    Abdullah Oğuz Topçuoğlu         & 7063561 & Maryna Dernovaia \\
    Ahmed Waleed Ahmed Badawy Shora & 7069708 & Jan-Hendrik Gindorf \\
    Yousef Mostafa Farouk Farag     & 7073030 & Thorben Johr \\
  \end{tabular}
\end{flushright}

% Please note: An exercise that asks you to design an algorithm is to be answered by (1) your
% algorithm in pseudocode, (2) a correctness proof, and (3) a running time analysis.
% Exercise 1 (Alternative Dictionary) 2+3+5 points
% Consider the following implementation of a dictionary data structure on n elements: We maintain
% arrays A0, A1, . . . , A⌈log n⌉
% , where Ai
% is either empty or contains exactly 2
% i
% elements in sorted order.
% The operation Insert is implemented as follows:
% 1: procedure Insert(x)
% 2: Let B be an array of length 1 with entry x
% 3: for i := 0, 1, . . . do
% 4: if Ai
% is empty then
% 5: Replace Ai by B and stop
% 6: else
% 7: B := Merge(Ai
% , B)
% 8: Replace Ai by an empty array
% It is your task to analyze the data structure.
% a. Start from an empty data structure (where all arrays Ai are empty), and apply the following
% operations: Insert(2), Insert(4), Insert(10), Insert(5), Insert(6), Insert(7). Draw the
% state of the arrays A0, A1, A2 after each operation.
% b. Implement an operation Find(k) that finds an element x in the data structure with x.key = k
% (or reports that there is no such element). Your implementation should run in time O
% (log n)
% 2
% 
% .
% c. Prove that Insert(x) runs in amortized time O(log n).
% Hint: Remember the amortized analysis of the binary counter.
% Exercise 2 (Many-Pop Stack) 4+4+5 points
% In the lecture we have seen the Stack data structure supporting the operations Push(x), Pop()
% and Front(), and we implemented Stacks using linked lists or dynamic arrays. Now we want to
% additionally support the operation ManyPop(k), which is given an integer k ≥ 1 and removes the
% topmost k items from the stack. If there are less than k items on the stack, then ManyPop(k) results
% in an empty stack.
% 1
% a. Suppose we know an upper bound N on the size of the stack, and we implement the stack as an
% array A[1..N]. In this setting, give an implementation of ManyPop(k) in worst-case time O(1).
% b. For stacks implemented via linked lists, implement ManyPop(k) in worst-case time O(k).
% c. For stacks implemented via linked lists, prove that the amortized running time of Push and
% ManyPop is O(1). That is, show that starting from an empty stack, any sequence σ1, . . . , σm of
% Push and ManyPop operations takes total time O(m).
% Exercise 3 (Manager’s Idea) 7 points
% Your manager asks you to change the PopBack() procedure of dynamic arrays to replace the line “if
% n = N/4 and n > 0 : Reallocate()” by “if n = N/2 − 1 and n > 0 : Reallocate()”. They argue
% that it is wasteful to shrink an array only when three-fourths of it are unused. Convince them that
% this is a bad idea, by giving a sequence of m PushBack and PopBack operations that would need
% time Θ(m2
% ) if their proposal was implemented.
% Exercise 4 (Dictionaries in Action) 5+5 points
% Suppose you have access to a dictionary data structure supporting the operations Find(x), Insert(x),
% Remove(x), and Min().
% a. Given an array A[1 .. n], compute the array B[1 .. m] obtained from A by removing all duplicates—
% that is, all elements which have appeared in the array before. (In particular, m is the number of
% distinct elements in A.) The order of the remaining elements should be unchanged.
% Design an algorithm for this problem using the dictionary data structure. Assuming that the
% dictionary operations run in time O(log n), your algorithm should run in time O(n log n).
% b. Argue that at least one of the operations Insert(x), Remove(x), and Min() requires time
% Ω(log n).
% Hint: Apply the lower bound we have seen for comparison-based sorting algorithms.
\section*{Exercise 2}
\subsection*{(a)}
The initial state: $A_0 =[],A_1=[], A_2=[]$\\
After insert(2): $A_0 =[2],A_1=[], A_2=[]$\\
After insert(4): $A_0 =[],A_1=[2,4], A_2=[]$\\
After insert(10): $A_0 =[10],A_1=[2,4], A_2=[]$\\
After insert(5): $A_0 =[],A_1=[], A_2=[2,4,5,10]$\\
After insert(6): $A_0 =[6],A_1=[], A_2=[2,4,5,10]$\\
After insert(7): $A_0 =[],A_1=[6,7], A_2=[2,4,5,10]$\\
\subsection*{(b)}

\textbf{Pseudocode:}

\begin{algorithm}
    \begin{algorithmic}[1]
        \Function{Find}{$k$}
            \For{$i := 0,1,...$}  \Comment{iterate through each array}
                \State $x = A_i.\text{BinarySearch}(k)$ \Comment{Binary search in sorted array $A_i$}
                \If{$x \neq \bot$}
                    \State \Return $x$
                \EndIf
            \EndFor
            \State \Return $\bot$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\textbf{Correctness proof:} 

- Each array $A_i$ is sorted. By the correctness of the binary search algorithm (from the lecture), 
  $A_i.\text{BinarySearch}(k)$ correctly returns the element of key $k$ if it exists in $A_i$, and $\bot$ otherwise.  
- Since the algorithm iterates over all arrays, it will find $k$ in the first array that contains it.  
- If $k$ is not present in any array, the algorithm correctly returns $\bot$.  

Hence, the algorithm correctly implements \textsc{Find}.

\medskip
\textbf{Time analysis:}

- Each array $A_i$ has size $|A_i| = 2^i$, for $i = 0, 1, \dots, \lceil \log n \rceil$.  
- Binary search on array $A_i$ takes $O(\log |A_i|) = O(\log 2^i) = O(i)$ time.  
- In the worst case, we may search all arrays, so the total time is:

\[
T_{\text{worst}} = \sum_{i=0}^{\lceil \log n \rceil} O(i) = O\Big( \sum_{i=0}^{\lceil \log n \rceil} i \Big) = O\big((\log n)^2\big)
\]

- Therefore, the worst-case running time of \textsc{Find} is $O((\log n)^2)$.

\section*{Exercise 2}
\subsection*{(a)}
We will have the Stack structure like this
\begin{verbatim}
struct Stack {
  int top; // index of the top element
  int A[N]; // array of size N
};
\end{verbatim}

So removing \(k\) many elements from stack can be done by just decrementing the top index by \(k\). Here is the pseudocode:
\begin{verbatim}
function ManyPop(Stack S, int k)
  if S.top < k then
    S.top = 0; // stack becomes empty
  else
    S.top -= k; // remove top k elements
\end{verbatim}

This implementation runs in \(O(1)\) time since it only involves a couple of arithmetic operations and a conditional check.

\subsection*{(b)}
We will have the Stack structure like this
\begin{verbatim}
struct Node {
  int data;
  Node* next;
};

struct Stack {
  Node* top; // pointer to the top node
};
\end{verbatim}

Every element on the stack points to the elements thats below it. So the first element thats pushed to the stack has its next pointer as null. Here is the pseudocode for ManyPop:
\begin{verbatim}
function ManyPop(Stack S, int k)
  count = 0;
  while S.top != null and count < k do
    temp = S.top;
    S.top = S.top.next; // move top to the next element
    delete temp; // free memory of the popped element
    count += 1;
\end{verbatim}

This implementation runs in \(O(k)\) time since it involves a loop that iterates \(k\) times, performing constant time operations in each iteration.

\subsection*{(c)}
To prove that the amortized running time of Push and ManyPop is \(O(1)\), we will use the bank accounting method from the lecture.
We will assign an amortized cost to each operation as follows:
\begin{itemize}
  \item Push operation: Lets say we get 2 euros every time Push() is called. We spend 1 euro for the constant time operation in Push() function and store
    the remaining 1 euro in the bank.
  \item ManyPop operation: Lets say we get 0 euro every time ManyPop() is called. We need k euros to remove k elements from the stack. Meaning that we need 1 euro to remove one element.
    Luckily enough we saved 1 euro per element during the Push() operations so we can spend them here.
\end{itemize}

\section*{Exercise 3}
This is not a great idea because when we shrink when \(n = N/2 - 1\) the new size would be \(2n = N - 2\). So we are just reducing the size by 2. And in the lecture we saw that adding
a constant amount when resizing(growing) is a bad idea so we can guess that something similar would happen when removing(shrinking) a constant amount too. \\
Consider this sequence
\begin{verbatim}
// we start with a single push back
PushBack(x1) // array count becomes 1 // capacity here is 1

// we continue with two push backs and then two pop backs and repeat this as many times as you want
PushBack(x2) // array count becomes 2 // capacity here is 2
PushBack(x3) // array count becomes 3 // capacity here is 4
PopBack()    // array count becomes 2
PopBack()    // array count becomes 1 // resizing happens here, new capacity becomes 2

PushBack(x4) // array count becomes 2
PushBack(x5) // array count becomes 3 // resizing happens here, new capacity becomes 4
PopBack()    // array count becomes 2
PopBack()    // array count becomes 1 // resizing happens here, new capacity becomes 2
\end{verbatim}

So every second operation we have a resizing and resizing is linear time. So if we have \(m\) operations we would have \(\frac{m}{2}\) resizings and each resizing takes linear time.
So the total time would be \(O(m^2)\).

\section*{Exercise 4}
\begin{itemize}
    \item \begin{algorithm}

\begin{algorithmic}[1]
\Function{RemoveDuplicate}{$A[1..n]$}
    \State Allocate $B[1..m]$
    \State $C := $ new Dictionary
    \State $j := 1$
    \For{$i := 1 \text{ to } n$} \Comment{$O(n)$}
        \If{$C.\text{find}(A[i]) = \bot$}
            \State $C.\text{insert}(A[i])$ \Comment{$O(\log n)$}
            \State $B[j] := A[i]$
            \State $j = j + 1$
        \EndIf
    \EndFor
    \State \Return $B$
\EndFunction
\end{algorithmic}
\end{algorithm}

    \textbf{Correctness Proof:}

We show that the algorithm \textsc{RemoveDuplicate} correctly returns an array $B$ 
that contains all elements of $A$ in their original order, with duplicates removed.

\medskip
\noindent
\textbf{Case 1:} $A$ has no duplicate elements.

\begin{itemize}
\item For each $i$ from $1$ to $n$, the lookup $C.\text{find}(A[i])$ will return $\bot$ 
because the element $A[i]$ has not been inserted before.
\item Therefore, the condition \texttt{if C.find(A[i]) = $\bot$} is always true, 
so every element $A[i]$ is inserted into both the dictionary $C$ and the output array $B$.
\item Consequently, $B$ will contain exactly all elements of $A$ in the same order, 
and no elements are skipped.
\end{itemize}

Hence, if $A$ has no duplicates, $B = A$.

\medskip
\noindent
\textbf{Case 2:} $A$ contains duplicates.

\begin{itemize}
\item Consider the first occurrence of any value $x$ in $A$. 
At this point, $C.\text{find}(x) = \bot$, 
so $x$ is inserted into $C$ and appended to $B$.
\item For any subsequent occurrence of the same value $x$, 
we have $C.\text{find}(x) \neq \bot$, 
so the algorithm skips the insertion and does not append $x$ to $B$.
\item Thus, each distinct element of $A$ appears exactly once in $B$, 
and since we traverse $A$ in order, the relative order of the first appearances is preserved.
\end{itemize}

\medskip
\noindent
\textbf{Conclusion:}
In both cases, $B$ contains exactly the distinct elements of $A$, 
preserving their order of first appearance. Therefore, 
the algorithm is \emph{correct}.

    \textbf{Time analysis:}
Each of the $n$ iterations performs one dictionary lookup and possibly one insertion, 
each taking $O(\log n)$ time, so the total running time is $O(n \log n)$.
\item
\textbf{1. Binary Search Tree (BST):} 

Let the BST have $n$ elements and height $h$. In a basic (unbalanced) BST, $h$ can be as large as $n$.  
- $\text{Insert}(x)$ requires descending from the root to the appropriate leaf: $\Theta(h)$ time.  
- $\text{Remove}(x)$ also requires searching for $x$ and possibly adjusting the tree: $\Theta(h)$.  
- $\text{Min}()$ requires following the leftmost path: $\Theta(h)$.  

Even in a balanced BST, $h = \Theta(\log n)$. Therefore, at least one of the operations $\text{Insert}$, $\text{Remove}$, or $\text{Min}$ requires $\Omega(\log n)$ time.

\medskip
\textbf{2. Sorted Array :} 

In a sorted array of size $n$:  
- $\text{Min}()$ can be performed in $\Theta(1)$ time (the first element).  
- $\text{Insert}(x)$ can find the correct position using binary search in $\Theta(\log n)$ time, 
  but it must shift all subsequent elements to make space: $\Theta(n)$ worst case.  
- $\text{Remove}(x)$ can locate the element using binary search in $\Theta(\log n)$ time, 
  but shifting elements after removal costs $\Theta(n)$ in the worst case.  

Hence, at least one operation ($\text{Insert}$ or $\text{Remove}$) requires $\Omega(n)$ time,
which certainly implies $\Omega(\log n)$.

\medskip
\textbf{3. Unsorted Array:} 

In an unsorted array:  
- $\text{Insert}(x)$ can be done in $\Theta(1)$ time (append at the end).  
- $\text{Remove}(x)$ requires locating $x$ by scanning the array: $\Theta(n)$.  
- $\text{Min}()$ requires scanning all $n$ elements: $\Theta(n)$.  

Thus, at least one operation ($\text{Remove}$ or $\text{Min}$) requires $\Omega(\log n)$ time.

\medskip
\textbf{4. Linked List:} 

For a singly linked list:  
- $\text{Insert}(x)$ at the head or tail can be done in $\Theta(1)$ time.  
- $\text{Remove}(x)$ requires scanning for $x$: $\Theta(n)$.  
- $\text{Min}()$ requires traversing all $n$ elements: $\Theta(n)$.  

Again, at least one operation requires $\Omega(\log n)$ time.
\end{itemize}
\end{document}

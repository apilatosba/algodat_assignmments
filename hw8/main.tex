\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{array}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}

\section*{\huge Homework Sheet 8}
\begin{flushright}
  \begin{tabular}{@{} l l l @{}}
    \textbf{Author} & \textbf{Matriculation Number} & \textbf{Tutor} \\
    Abdullah Oğuz Topçuoğlu         & 7063561 & Maryna Dernovaia \\
    Ahmed Waleed Ahmed Badawy Shora & 7069708 & Jan-Hendrik Gindorf \\
    Yousef Mostafa Farouk Farag     & 7073030 & Thorben Johr \\
  \end{tabular}
\end{flushright}

% --------------------------------------------------------------------
% ----------------------------- EXERCISE 1 ---------------------------
% --------------------------------------------------------------------

\section*{Exercise 1 (Coin Flips)}

\begin{enumerate}[label=\textbf{\alph*.}]

% (a)
\item \textbf{}

This is deterministic. She always stops after exactly $n$ flips.
\[
\mathbb{E}[T] = n.
\]

% (b)
\item \textbf{}

Let $T$ be the time of the first head. This is a geometric random variable with success probability $p=\tfrac12$:
\[
\mathbb{E}[T] = \frac{1}{p} = 2.
\]

% (c)
\item \textbf{}

This is the negative-binomial stopping time for $n$ successes with success probability $p=\tfrac12$:
\[
\mathbb{E}[T] = \frac{n}{p} = 2n.
\]

% (d)
\item \textbf{}

Let $E_0,E_1,E_2$ denote the expected remaining flips given that the current run of consecutive tails has length $0,1,2$.  
We stop when we reach a run of $3$ tails, so $E_3=0$.

We set up recurrences:
\[
\begin{aligned}
E_0 &= 1 + \tfrac12 E_1 + \tfrac12 E_0,\\[4pt]
E_1 &= 1 + \tfrac12 E_2 + \tfrac12 E_0,\\[4pt]
E_2 &= 1 + \tfrac12 \cdot 0 + \tfrac12 E_0.
\end{aligned}
\]

Solving these:
\[
E_2 = 1 + \tfrac12 E_0, \qquad
E_0 = 2 + E_1.
\]
Substitute into the equation for $E_1$:
\[
E_1 = 1 + \tfrac12(1 + \tfrac12 E_0) + \tfrac12 E_0
     = \tfrac32 + \tfrac34 E_0.
\]
Thus,
\[
E_0 = 2 + \tfrac32 + \tfrac34 E_0
\quad\Rightarrow\quad
E_0 - \tfrac34 E_0 = \tfrac74
\quad\Rightarrow\quad
E_0 = 14.
\]

Hence the expected number of flips until three consecutive tails appear is
\[
\mathbb{E}[T] = 14.
\]

\end{enumerate}
% --------------------------------------------------------------------
% ----------------------------- EXERCISE 2 ---------------------------
% --------------------------------------------------------------------

\section*{Exercise 2}
\begin{enumerate}
    \item \begin{verbatim}
    Solution (A[1,..n]):
        repeat:
            i := rand(n)
            if A[i] = 1: return i
    \end{verbatim}
    \textbf{Correctness Proof:} \\
    from the lecture, it is correct.\\
    \textbf{Time Analysis:}\\
Let $T$ be the number of iterations of the loop.  
In each iteration we pick a uniformly random index $i \in \{1,\dots,n\}$.
Since the array contains exactly $k$ ones, we have
\[
\Pr[A[i] = 1] = \frac{k}{n}
\qquad\text{and}\qquad
\Pr[A[i] = 0] = 1 - \frac{k}{n}.
\]

We compute $\mathbb{E}[T]$ using an indirect argument.  
We condition on the outcome of the first iteration:
\[
\mathbb{E}[T]
    = \Pr[A[i] = 1]\cdot 1
      + \Pr[A[i] = 0]\cdot (1 + \mathbb{E}[T']),
\]
where $T'$ is the number of \emph{additional} iterations if the first attempt fails.
Since $T$ and $T'$ have the same distribution, we substitute $T' \sim T$:
\[
\mathbb{E}[T]
    = \frac{k}{n}\cdot 1
      + \Bigl(1 - \frac{k}{n}\Bigr)\cdot (1 + \mathbb{E}[T]).
\]

Expanding and solving:
\[
\mathbb{E}[T]
    = \frac{k}{n} + \Bigl(1 - \frac{k}{n}\Bigr)
      + \Bigl(1 - \frac{k}{n}\Bigr)\mathbb{E}[T]
    = 1 + \Bigl(1 - \frac{k}{n}\Bigr)\mathbb{E}[T].
\]

Rearranging:
\[
\frac{k}{n}\,\mathbb{E}[T] = 1
\qquad\Longrightarrow\qquad
\mathbb{E}[T] = \frac{n}{k}.
\]

Each iteration takes $O(1)$ time, so the expected running time is O(n/k).

   \item \begin{verbatim}
    Solution (A[1,..n], k):
          c := 1
          repeat:
            i := rand(n)
            if A[i] = 1: return 2
            if 1- (1 - k/n)^c >= 0.999: return 1
            c:= c+1
    \end{verbatim}
\textbf{Correctness Proof:}\\

We analyze the behavior of the algorithm in the two cases specified in the problem.

\paragraph{Case 1: $A$ contains only zeros.}
In this case, every sampled index $i$ satisfies $A[i] = 0$, so the algorithm never returns~2.
The algorithm terminates only when
\[
1 - (1 - k/n)^c \ge 0.999,
\]
and at that point it returns~1.  
Thus, in the all-zero case the algorithm outputs~1 with probability~1,
which is at least the required~$0.999$.

\paragraph{Case 2: $A$ contains at least $k$ ones.}
In each iteration, a uniformly random index is sampled.  
Since the array contains at least $k$ ones, the probability of sampling a~1 in
any given iteration is at least $k/n$.
Let $c$ be the number of iterations performed.
The probability that the algorithm \emph{fails} to see any 1 during the first $c$ samples is at most
\[
(1 - k/n)^c.
\]
The algorithm stops and outputs~1 only if
\[
1 - (1 - k/n)^c \ge 0.999,
\quad\text{ i.e., }\quad
(1 - k/n)^c \le 0.001.
\]
Thus the probability that the algorithm incorrectly outputs~1 in this case is at most $0.001$.
Otherwise, it must have encountered a position $i$ such that $A[i]=1$,
and it outputs~2.
Therefore, when $A$ contains at least $k$ ones, the algorithm outputs~2
with probability at least $0.999$.

\medskip
Combining both cases, the algorithm satisfies the required $0.999$ success probability.\\
\textbf{Time analysis:}\\
If the algorithm encounters a 1 then it would be the same as the one above.
If the algorithm does not encounter a 1, then by the stopping condition we have
\[
(1 - k/n)^c \le 0.001.
\]
Taking natural logarithms of both sides gives
\[
c\ln(1 - k/n) \le \ln(0.001).
\]
Note that \(\ln(1 - k/n)<0\), so dividing by \(\ln(1 - k/n)\) (which reverses no sign here because both sides are negative) yields
\[
c \ge \frac{\ln(0.001)}{\ln(1 - k/n)}.
\]
We now use the inequality \(\ln(1-x)\le -x\) valid for \(0<x<1\). With \(x=k/n\) this gives \(\ln(1-k/n)\le -k/n\). Hence
\[
\frac{\ln(0.001)}{\ln(1 - k/n)}
\ge \frac{\ln(0.001)}{-\,k/n}
= \frac{n}{k}\,\ln(1000).
\]
Therefore
\[
c \ge \frac{n}{k}\,\ln(1000) = \Theta\!\left(\frac{n}{k}\right).
\]
Each iteration performs \(O(1)\) work (one random index and one array access), so the total running time is
\[
c\cdot O(1)=O\!\left(\frac{n}{k}\right).
\]
Thus the algorithm runs in \(O(n/k)\) time.

\end{enumerate}
% --------------------------------------------------------------------
% ----------------------------- EXERCISE 3 ---------------------------
% --------------------------------------------------------------------

\section*{Exercise 3}

\begin{verbatim}
void Permute(A[1..n])
   if (n == 1) return
   int randomIndex = rand(n) // rand() function from the lecture
   swap(A[1], A[randomIndex])
   Permute(A[2..n])
\end{verbatim}

\textbf{Correctness Proof:} \\
We prove by induction that the algorithm produces a uniform random permutation of the array $A[1..n]$.

\textbf{Base Case:} For $n = 1$, there is only one permutation. The algorithm returns the array unchanged.

\textbf{Inductive Step:} Assume the algorithm produces a uniformly random permutation for arrays of size $k-1$.  
For size $k$, the algorithm selects a random index from $1$ to $k$, placing each element in the first position with probability $1/k$.  
Then it recursively permutes the remaining $k-1$ elements uniformly by the induction hypothesis.  
Thus the resulting permutation is uniform.

\textbf{Running Time:}
\[
T(1)=1,\qquad T(n)=T(n-1)+1,
\]
so the running time is
\[
T(n) = O(n).
\]
\section*{Exercise 4}
\begin{enumerate}
    \item 
\begin{verbatim}
Corrupted(A[(x1,y1), (x2,y2) .... (xn,yn)])
   for i := 1,2,... n:
      for j := i + 1, ... n:
         a := (A[i].second - A[j].second) / (A[i].first - A[j].first)
         b := A[i].second - a * A[i].first
         count := 0
         for k := 1, 2 ,...n:
            if A[k].second = a * A[k].first + b:
              count := count + 1
            if count >= n/2+1:
              return (a,b)
\end{verbatim}
\textbf{Correctness Proof:}

We are given that at least $\frac{n}{2} + 1$ of the input points 
$(x_i, y_i)$ are non--corrupted, meaning that for these points
\[
y_i = ax_i + b
\]
for the true underlying coefficients $a,b$.  

\medskip
\noindent
\textit{(1) If the algorithm outputs $(a,b)$, then it is the correct line.}

The algorithm returns $(a,b)$ only if the computed line
\[
y = ax + b
\]
agrees with at least $\frac{n}{2}+1$ of the points.  
By assumption, the true line agrees with at least $\frac{n}{2}+1$ points 
(the non--corrupted ones).  
Any incorrect line can agree with at most $\frac{n}{2}$ points, since 
there are fewer than $\frac{n}{2}$ corrupted points.  
Therefore, if a line agrees with $\frac{n}{2}+1$ points, it must be the
unique correct line.  
Hence, whenever the algorithm returns $(a,b)$, the returned line is correct.

\medskip
\noindent
\textit{(2) The algorithm is guaranteed to find the correct line.}

Since more than half of the points are non--corrupted, there exist at least
\[
\binom{\frac{n}{2}+1}{2}
\]
distinct pairs $(i,j)$ of non--corrupted points.  
For any such pair, the algorithm computes
\[
a = \frac{y_j - y_i}{x_j - x_i}, \qquad 
b = y_i - ax_i,
\]
which are exactly the true coefficients of the underlying line.  
When the algorithm checks this line against all $n$ points, it will find 
at least $\frac{n}{2}+1$ matches and therefore return the correct pair
$(a,b)$.  
Thus, because the algorithm examines \emph{all} pairs $(i,j)$, it is certain
to eventually test a pair of non--corrupted points and return the correct line.

\medskip
\noindent
\text{Therefore, the algorithm always returns the correct coefficients $(a,b)$.}

\bigskip
\textbf{Time Analysis:}

The algorithm consists of three nested loops.  
The outer two loops iterate over all pairs $(i,j)$ with $1 \le i < j \le n$,
which is
\[
\sum_{i=1}^n (n-i)
= \frac{n(n-1)}{2}
= O(n^2)
\]
pairs.  
For each pair, the algorithm checks all $n$ points to count how many lie on the
candidate line, which takes $O(n)$ time.

Thus, the total running time is
\[
O(n^2) \cdot O(n) = O(n^3).
\]

\medskip
\noindent
\textbf{Therefore, the overall running time of the algorithm is $O(n^3)$.}
\item\begin{verbatim}
CorruptedRand(A[(x1,y1), (x2,y2) .... (xn,yn)])
     repeat:    
         i:= Rand(n)
         j:= Rand(n)
         if i != j:
           a := (A[i].second - A[j].second) / (A[i].first - A[j].first)
           b := A[i].second - a * A[i].first
           count := 0
           for k := 1, 2 ,...n:
              if A[k].second = a * A[k].first + b:
                count := count + 1
              if count >= n/2+1:
                return (a,b)
         
\end{verbatim}
\textbf{Correctness Proof:}

We are given that at least $\frac{n}{2}+1$ of the input points 
$(x_i,y_i)$ satisfy the true linear relation
\[
y_i = ax_i + b,
\]
while the remaining points may be arbitrary.  

\medskip
\noindent
\textit{(1) If the algorithm outputs $(a,b)$, then the output is correct.}

The algorithm returns $(a,b)$ only when the line $y=ax+b$ agrees with 
at least $\frac{n}{2}+1$ points in the input.  
By assumption, the true line agrees with all non--corrupted points, and there are at least 
$\frac{n}{2}+1$ such points.  
Conversely, any incorrect line can agree with at most the number of corrupted points, which is 
strictly less than $\frac{n}{2}$.  
Therefore, a line that matches $\frac{n}{2}+1$ or more points must be the unique correct line.
Hence the algorithm can only return the correct coefficients.

\medskip
\noindent
\textit{(2) The algorithm finds the correct line with constant probability on each iteration.}

On each recursive iteration, the algorithm chooses two indices $i$ and $j$ independently and uniformly
at random from $\{1,\dots,n\}$.  
Since more than half the points are non--corrupted, the probability that both chosen points 
are non--corrupted is
\[
p \;=\; 
\frac{\frac{n}{2}+1}{n} \cdot \frac{\frac{n}{2}}{n-1}
\;=\;
\Theta(1).
\]
If both points are non--corrupted, then the computed values
\[
a = \frac{y_j - y_i}{x_j - x_i}, \qquad
b = y_i - ax_i,
\]
are exactly the true coefficients of the underlying line.  
In this case the line agrees with all non--corrupted points and therefore 
with at least $\frac{n}{2}+1$ points.  
Thus the algorithm will return $(a,b)$ and terminate.

\smallskip
If one or both of the chosen points are corrupted, the line constructed is incorrect and cannot
agree with $\frac{n}{2}+1$ points.  
In this case the algorithm rejects the line and repeats the process.  
Because the probability of success on each iteration is a positive constant $p$, 
the expected number of iterations before success is 
\[
\mathbb{E}[\text{iterations}] = \frac{1}{p} = O(1).
\]

\medskip
\noindent
Thus the randomized algorithm always returns the correct line and succeeds in a constant expected number of iterations.

\bigskip
\textbf{Time Analysis:}

Each iteration of the algorithm performs the following work:

\begin{itemize}
    \item randomly selects two indices $i$ and $j$, which takes $O(1)$ time;
    \item computes $a$ and $b$, which takes $O(1)$ time;
    \item scans all $n$ points to check how many lie on the resulting line, which takes $O(n)$ time.
\end{itemize}

Thus each iteration costs $O(n)$ time.  
Since the algorithm succeeds with constant probability on each iteration, the expected number of 
iterations before termination is $O(1)$.  
Therefore, the total expected running time is
\[
O(1) \cdot O(n) = O(n).
\]

\medskip
\noindent
\textbf{Hence the randomized algorithm runs in expected time $O(n)$}.

\end{enumerate}

\end{document}
